\subsection{Implementation}

The Social Network Exploration platform is developed on the Java Virtual Machine technology.  We tap into fast and robust databases for storing the vast amounts of data coming daily -- the Berkeley DB (BDB) \cite{BDB}, an in-process persistent hash, and MongoDB \cite{MongoDB}, a JSON-based client/server with in-memory cache and excellent Java driver.  We used two modern JVM languages, Scala and Clojure, for efficient data mining.
Scala is a modern object-functional language, allowing for compact and expressive code while fully interoperable with the Java platform.  We previously have successfully developed data mining software with the Functional Programming (FP) paradigm, and Scala allows to use FP smoothly with new code and existing libraries.

The communication graph is represented as a series of adjacency lists, storing repliers for each source node.  An alternative would be to store triplets \verb|<source,target,weight>|, where weight is any object decorating the edge.  When using SQL databases, the triplets is pretty much all you can do.  But since we are using Berkeley DB Java Edition, capable of storing a variety of JVM objects, we store the adjacency lists directly as hash maps.  This makes it significantly faster to get all the repliers for a particular user.  We use Java-Scala interoperability to store the graph as Java hash maps, allowing them to be retrieved from any JVM language.  This experience lead to optimizing Berkeley DB with the Oracle team, as reported on its blog \cite{OracleBlog}.

\subsection{Natural Language Processing}

For the n-gram models and SIPs, we used the LingPipe \cite{LingPipe}, an NLP library written in Java.  It comes with very well written tutorials on various NLP tasks, built-in tokenizers, and serialization.  For text search, we use Lucene, the premier full text search engine originally and currently developed in Java.  The n-gram models are well-studied in NLP, and they must be compact, fast, and serializable.  LingPipe stores both a binary form for efficient perplexity computation and Google n-gram format for full count access.
