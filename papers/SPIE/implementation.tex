\subsection{Implementation}

The Social Network Exploration platform is developed on the Java Virtual Machine technology.  We tap into fast and robust databases for storing the vast amounts of data coming daily -- the Berkeley DB (BDB) \cite{code:berkeleydb}, an in-process persistent hash, and MongoDB \cite{code:mongodb}, a JSON-based client/server with in-memory cache and excellent Java driver.  We used two modern JVM languages, \emph{Scala} \cite{code:scala} and \emph{Clojure} \cite{code:clojure}, for efficient data mining.  Scala is a modern object-functional language, allowing for compact and expressive code while fully interoperable with the Java platform.  We previously have successfully developed data mining software with the Functional Programming (FP) paradigm, and Scala FP blends with superior object orientation and interoperability.

The communication graph is represented as a series of adjacency lists, storing repliers for each source node.  An alternative would be to store triplets \verb|<source,target,weight>|, where weight is any object decorating the edge.  When using SQL databases, the triplets are pretty much all you can do.  But since we are using Berkeley DB Java Edition, capable of storing a variety of JVM objects, we store the adjacency lists directly as hash maps.  This makes it significantly faster to get all the repliers for a particular user.  We use Java-Scala interoperability to store the graph as Java hash maps, allowing them to be retrieved from any JVM language.  This experience lead to optimizing Berkeley DB for Twitter storage by the first author with the Oracle team directly, as reported on the Oracle Java blog \cite{OracleBlog:Khrabrov}.

\subsection{Natural Language Processing}

For the n-gram models and SIPs, we used \emph{LingPipe} \cite{code:lingpipe}, an NLP library written in Java.  It comes with very well written tutorials on various NLP tasks, built-in tokenizers, and serialization.  For text search, we use Lucene, the premier full text search engine originally and currently developed in Java.  The n-gram models are well-studied in NLP, and they must be compact, fast, and serializable.  LingPipe stores both a binary form for efficient perplexity computation and Google n-gram format for full count access.

The code for this paper is open source and available on the GitHub as the \emph{Tfitter} project \cite{code:tfitter}.
